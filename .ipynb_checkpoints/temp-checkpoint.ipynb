{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "root_path = 'data/transcriptions'\n",
    "data_path = os.path.join(root_path,'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "characters = set()\n",
    "transcripts = set()\n",
    "\n",
    "with open(data_path) as file:\n",
    "    rows = file.readlines()\n",
    "    for i,row in enumerate(rows[1:]):\n",
    "        splitted_data = row.split(',')\n",
    "        data = splitted_data[1].strip()\n",
    "        target = splitted_data[2].strip().replace(' ','')\n",
    "        \n",
    "        for character in data:\n",
    "            characters.add(character)\n",
    "        for transcript in target:\n",
    "            transcripts.add(transcript)\n",
    "        full_data = {'data':data,'target':target}\n",
    "                \n",
    "        if i > 80000:\n",
    "            test_data.append(full_data)\n",
    "            continue\n",
    "        train_data.append(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2char_characters = dict(enumerate(characters,start=1))\n",
    "char2int_characters = {ch:ind for ind,ch in int2char_characters.items()}\n",
    "int2char_transcripts = dict(enumerate(transcripts,start=1))\n",
    "char2int_transcripts = {ch:ind for ind,ch in int2char_transcripts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_data_x = []\n",
    "encoded_train_data_y = []\n",
    "for word in train_data:\n",
    "    encoded_train_data_x.append([char2int_characters[ch] for ch in word['data']])\n",
    "    encoded_train_data_y.append([char2int_transcripts[ch] for ch in word['target']])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x,arr_y, batch_size):\n",
    "   \n",
    "    batch = {'x':[],'y':[]}\n",
    "    for i in range(batch_size):\n",
    "        rand = int(random()*len(arr_x))\n",
    "        batch['x'].append(arr_x[rand])\n",
    "        batch['y'].append(arr_y[rand])\n",
    "        \n",
    "    return batch\n",
    "\n",
    "def normalize_batch(x,y):\n",
    "    x_lengths = []\n",
    "    y_lengths = []\n",
    "    x = sorted(x,key=len,reverse=True)\n",
    "    y = sorted(y,key=len,reverse=True)\n",
    "    max1 = len(x[0])\n",
    "    max2 = len(y[0])\n",
    "    for i in range(len(x)):\n",
    "        x_lengths.append(len(x[i]))\n",
    "        y_lengths.append(len(y[i]))\n",
    "        \n",
    "        for _ in range(max1-len(x[i])):\n",
    "            x[i].append(0)\n",
    "        for _ in range(max2-len(y[i])):\n",
    "            y[i].append(0)\n",
    "    return x,y,x_lengths,y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'data/transcriptions/train.csv'\n",
    "characters = pd.read_csv(file_path,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter,for_encoder=False, min_freq=None, max_freq=None):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        if for_encoder:\n",
    "            self._token2idx = {\n",
    "                self.pad:self.pad_idx,\n",
    "                self.unk:self.unk_idx,\n",
    "            }\n",
    "        else:\n",
    "            self._token2idx = {\n",
    "                self.sos: self.sos_idx,\n",
    "                self.eos: self.eos_idx,\n",
    "                self.pad: self.pad_idx,\n",
    "                self.unk: self.unk_idx,\n",
    "            }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        max_freq = len(counter) if max_freq is None else max_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq and count < max_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def sent2idx(self, sent):\n",
    "        return [self.token2idx(i) for i in sent]\n",
    "    \n",
    "    def idx2sent(self, idx):\n",
    "        return [self.idx2token(i) for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return '{}'.format(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharactersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file_path,transform = None):\n",
    "        self.file = pd.read_csv(csv_file_path,'r')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.characters_vocab = None\n",
    "        self.transcripts_vocab = None\n",
    "        self.make_dataset()\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.data[idx]['x']\n",
    "        y = self.data[idx]['y']\n",
    "        data = {'x':x,'y':y}\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        characters = set()\n",
    "        transcripts = set()\n",
    "        non_needed_symbold = '\\'#$?\\\\_({)}-:\\\";!%.1234567890'\n",
    "        for idx in range(len(self.file)):\n",
    "            item = str(self.file.iloc[idx][0]).split(',')\n",
    "            \n",
    "            x = item[1].strip()\n",
    "            for symbol in non_needed_symbold:\n",
    "                x = x.replace(symbol,'')\n",
    "            y = item[2].replace(' ','')\n",
    "            self.data.append({'x':x,'y':y})\n",
    "            for character in x:\n",
    "                characters.add(character)\n",
    "            for transcript in y:\n",
    "                transcripts.add(transcript)\n",
    "        \n",
    "        self.characters_vocab = Vocab({v:k for k,v in dict(enumerate(characters)).items()},for_encoder=True)\n",
    "        self.transcripts_vocab = Vocab({v:k for k,v in dict(enumerate(transcripts)).items()})\n",
    "        \n",
    "            \n",
    "    def collate_fn(self, batch): \n",
    "        x_values = []\n",
    "        y_values_in = []\n",
    "        x_lengths = []\n",
    "        y_lengths = []\n",
    "        for item in batch:\n",
    "            \n",
    "            x_values.append([self.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "            y_values_in.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "        \n",
    "        x_values = sorted(x_values,key=len,reverse=True)\n",
    "        y_values_in = sorted(y_values_in,key=len,reverse=True)\n",
    "        \n",
    "        max_x = len(x_values[0])\n",
    "        max_y = len(y_values_in[0])\n",
    "        \n",
    "        for word_index in range(len(x_values)):\n",
    "            \n",
    "            x_lengths.append(len(x_values[word_index]))\n",
    "            y_lengths.append(len(y_values_in[word_index]))\n",
    "            \n",
    "            for _ in range(max_x - len(x_values[word_index])):\n",
    "                x_values[word_index].append(0)\n",
    "            for _ in range(max_y - len(y_values_in[word_index])):\n",
    "                y_values_in[word_index].append(0)\n",
    "            \n",
    "            y_values_in[word_index].insert(0,2)\n",
    "            \n",
    "        x_values = torch.tensor(x_values)\n",
    "        y_values_in_tensor = torch.tensor(y_values_in)\n",
    "        \n",
    "        y_values_out = y_values_in        \n",
    "        for arr_index in range(len(y_values_out)):\n",
    "            y_values_out[arr_index] = y_values_out[arr_index][1:] + [3]\n",
    "        y_values_out_tensor = torch.tensor(y_values_out)\n",
    "        \n",
    "        return x_values,y_values_in_tensor,y_values_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = CharactersDataset(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True,num_workers=2,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    kek = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': 'GRIN', 'y': 'GRIHN'},\n",
       " {'x': 'MARANISS', 'y': 'MERAENIHS'},\n",
       " {'x': 'LOFFREDO', 'y': 'LOWFREYDOW'},\n",
       " {'x': 'FINDLING', 'y': 'FIHNDAHLIHNG'},\n",
       " {'x': 'ISAKSEN', 'y': 'IHSAHKSAHN'},\n",
       " {'x': 'OVERRATE', 'y': 'OWVERREYT'},\n",
       " {'x': 'ATHEARN', 'y': 'EYTHERN'},\n",
       " {'x': 'PRECEDING', 'y': 'PRIYSIYDIHNG'},\n",
       " {'x': 'BRUTALITY', 'y': 'BRUWTAELIHTIY'},\n",
       " {'x': 'CULLOM', 'y': 'KAHLAHM'},\n",
       " {'x': 'CECILE', 'y': 'SIHSIYL'},\n",
       " {'x': 'HOPES', 'y': 'HHOWPS'},\n",
       " {'x': 'KAYA', 'y': 'KAAYAH'},\n",
       " {'x': 'CABLES', 'y': 'KEYBAHLZ'},\n",
       " {'x': 'REGRETTING', 'y': 'RIHGREHTIHNG'},\n",
       " {'x': 'CHAZOV', 'y': 'CHAEZAAV'},\n",
       " {'x': 'SUPERVISE', 'y': 'SUWPERVAYZ'},\n",
       " {'x': 'STANBERY', 'y': 'STAENBERIY'},\n",
       " {'x': 'BAGMAN', 'y': 'BAEGMAHN'},\n",
       " {'x': 'ANGER', 'y': 'AENGGER'},\n",
       " {'x': 'BANKROLL', 'y': 'BAENGKROWL'},\n",
       " {'x': 'BRACELET', 'y': 'BREYSLAHT'},\n",
       " {'x': 'GAHLI', 'y': 'GAALIY'},\n",
       " {'x': 'SPORTSCHANNEL', 'y': 'SPAORTSCHAENAHL'},\n",
       " {'x': 'JIMMERSON', 'y': 'JHIHMERSAHN'},\n",
       " {'x': 'ECOLOGISTS', 'y': 'IYKAALAHJHIHSTS'},\n",
       " {'x': 'EGE', 'y': 'IYJH'},\n",
       " {'x': 'BEACHUM', 'y': 'BIYCHAHM'},\n",
       " {'x': 'PRONTO', 'y': 'PRAANTOW'},\n",
       " {'x': 'POLYTECHNIC', 'y': 'PAALIYTEHKNIHK'},\n",
       " {'x': 'OVERCHARGE', 'y': 'OWVERCHAARJH'},\n",
       " {'x': 'TUK', 'y': 'TUWK'}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values,y_values = [],[]\n",
    "for item in kek:\n",
    "    x_values.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in kek:\n",
    "    print(item['x'])\n",
    "    print(item['y'])\n",
    "    x_values.append([dataset.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "    y_values.append([dataset.transcripts_vocab.token2idx(tr) for tr in item['y']])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.characters_vocab.token2idx(6234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                           sampler=train_sampler,collate_fn = dataset.collate_fn)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=valid_sampler,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,dict_len,hidden_size,n_layers = 1,dropout=0):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(dict_len,self.hidden_size,padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq,hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        #packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths)\n",
    "        outputs,(hidden,cell) = self.LSTM(embedded)\n",
    "        #outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size,n_layers=1,dropout=0):\n",
    "        super(DecoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_size,hidden_size, padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(hidden_size,hidden_size,n_layers,dropout = (0 if n_layers == 1 else self.dropout),batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_step,last_hidden, last_cell):\n",
    "        #input_step (batch_size,seq_len)\n",
    "        embedded = self.embedding(input_step)\n",
    "        #embedded(batch_size,seq_len,hidden_dim)\n",
    "        output,(hidden,cell) = self.LSTM(embedded,(last_hidden, last_cell))\n",
    "        #output(batch_size,seq_len,hidden_dim)\n",
    "        prediction = self.out(output)\n",
    "        #prediction(batch_size,seq_len,output_dim)\n",
    "        return prediction,hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(len(dataset.characters_vocab),128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderLSTM(128,len(dataset.transcripts_vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self,x,y,teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        \n",
    "        hidden,cell = self.encoder(x)\n",
    "\n",
    "        output,hidden,cell = self.decoder(y,hidden,cell)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predict(self,x):\n",
    "\n",
    "        #batch_size = 1!\n",
    "        hidden,cell = self.encoder(x)\n",
    "        out_input = torch.LongTensor([[2]]).to(device)\n",
    "        preds = []\n",
    "        while True:\n",
    "            output, hidden, cell = self.decoder(out_input, hidden, cell)\n",
    "            output = torch.argmax(output)\n",
    "            our_value = output.item()\n",
    "            if our_value == 3:\n",
    "                break\n",
    "            preds.append(our_value)\n",
    "            out_input = output.unsqueeze(0).unsqueeze(0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq(encoder,decoder,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): EncoderLSTM(\n",
       "    (embedding): Embedding(27, 128, padding_idx=0)\n",
       "    (LSTM): LSTM(128, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): DecoderLSTM(\n",
       "    (embedding): Embedding(27, 128, padding_idx=0)\n",
       "    (LSTM): LSTM(128, 128, batch_first=True)\n",
       "    (out): Linear(in_features=128, out_features=27, bias=True)\n",
       "    (softmax): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 274,587 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    kek = batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x = batch[0].to(device)\n",
    "        y_in = batch[1].to(device)\n",
    "        y_out = batch[2].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x, y_in)\n",
    "        #output dim (y_seq_len,batch_size,output_dim)\n",
    "        output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "        y_out = y_out.view(-1)\n",
    "        loss = criterion(output, y_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            y_in = batch[1].to(device)\n",
    "            y_out = batch[2].to(device)\n",
    "            \n",
    "            output = model(x,y_in) #turn off teacher forcing\n",
    "            \n",
    "            output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "            y_out = y_out.view(-1)\n",
    "\n",
    "            loss = criterion(output, y_out)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\tTrain Loss: 1.667 | Train PPL:   5.296\n",
      "10\n",
      "\tTrain Loss: 1.509 | Train PPL:   4.522\n",
      "10\n",
      "\tTrain Loss: 1.471 | Train PPL:   4.354\n",
      "10\n",
      "\tTrain Loss: 1.446 | Train PPL:   4.248\n",
      "10\n",
      "\tTrain Loss: 1.428 | Train PPL:   4.170\n",
      "10\n",
      "\tTrain Loss: 1.413 | Train PPL:   4.109\n",
      "10\n",
      "\tTrain Loss: 1.401 | Train PPL:   4.061\n",
      "10\n",
      "\tTrain Loss: 1.391 | Train PPL:   4.018\n",
      "10\n",
      "\tTrain Loss: 1.382 | Train PPL:   3.984\n",
      "10\n",
      "\tTrain Loss: 1.374 | Train PPL:   3.950\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(N_EPOCHS)\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, validation_loader, criterion)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(validation_loader):\n",
    "    kek = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'encoder_weights')\n",
    "torch.save(decoder.state_dict(),'decoder_weights')\n",
    "torch.save(model.state_dict(),'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(len(dataset.characters_vocab),128).to(device)\n",
    "decoder = DecoderLSTM(128,len(dataset.transcripts_vocab)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('encoder_weights'))\n",
    "decoder.load_state_dict(torch.load('decoder_weights'))\n",
    "model = seq2seq(encoder,decoder,device)\n",
    "model.load_state_dict(torch.load('model_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in validation_loader:\n",
    "    kek = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = kek[0][0][1:].unsqueeze(0).to(device)\n",
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<pad>': 0, '<unk>': 1, 'P': 2, 'N': 3, 'X': 4, 'J': 5, 'W': 6, 'L': 7, 'T': 8, 'Z': 9, 'K': 10, 'B': 11, 'E': 12, 'C': 13, 'V': 14, 'H': 15, 'O': 16, 'S': 17, 'D': 18, 'G': 19, 'R': 20, 'Y': 21, 'M': 22, 'U': 23, 'I': 24, 'F': 25, 'Q': 26},\n",
       " {'<sos>': 2, '<eos>': 3, '<pad>': 0, '<unk>': 1, 'P': 4, 'N': 5, 'J': 6, 'W': 7, 'L': 8, 'T': 9, 'Z': 10, 'K': 11, 'B': 12, 'E': 13, 'C': 14, 'V': 15, 'H': 16, 'O': 17, 'S': 18, 'D': 19, 'G': 20, 'R': 21, 'Y': 22, 'M': 23, 'U': 24, 'I': 25, 'F': 26})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.characters_vocab,dataset.transcripts_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_token = ''\n",
    "for pred in y_pred:\n",
    "    output_token += dataset.transcripts_vocab.idx2token(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_token = ''\n",
    "for x_val in x.squeeze(0):\n",
    "    input_token += dataset.characters_vocab.idx2token(x_val.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18, 22, 24,  3, 24, 17,  8, 20,  0,  8, 24, 14, 12,  7, 21]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in x.squeeze(0):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
