{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "root_path = 'data/transcriptions'\n",
    "data_path = os.path.join(root_path,'train.csv')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter,for_encoder=False, min_freq=None):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        if for_encoder:\n",
    "            self._token2idx = {\n",
    "                self.pad:self.pad_idx,\n",
    "                self.unk:self.unk_idx,\n",
    "            }\n",
    "        else:\n",
    "            self._token2idx = {\n",
    "                self.sos: self.sos_idx,\n",
    "                self.eos: self.eos_idx,\n",
    "                self.pad: self.pad_idx,\n",
    "                self.unk: self.unk_idx,\n",
    "            }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def sent2idx(self, sent):\n",
    "        return [self.token2idx(i) for i in sent]\n",
    "    \n",
    "    def idx2sent(self, idx):\n",
    "        return [self.idx2token(i) for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return '{}'.format(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharactersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file_path,transform = None):\n",
    "        self.file = pd.read_csv(csv_file_path,'r')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.characters_vocab = None\n",
    "        self.transcripts_vocab = None\n",
    "        self.non_needed_symbols = '\\'#$?\\\\_({)}-:\\\";!%.1234567890'\n",
    "        \n",
    "        self.make_dataset()\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.data[idx]['x']\n",
    "        y = self.data[idx]['y']\n",
    "        data = {'x':x,'y':y}\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        characters = set()\n",
    "        transcripts = set()\n",
    "        \n",
    "        for idx in range(len(self.file)):\n",
    "            item = str(self.file.iloc[idx][0]).split(',')\n",
    "            \n",
    "            x = item[1].strip()\n",
    "            for symbol in self.non_needed_symbols:\n",
    "                x = x.replace(symbol,'')\n",
    "            if x == '':\n",
    "                continue\n",
    "            y = item[2].replace(' ','')\n",
    "            self.data.append({'x':x,'y':y})\n",
    "            for character in x:\n",
    "                characters.add(character)\n",
    "            for transcript in y:\n",
    "                transcripts.add(transcript)\n",
    "        \n",
    "        self.characters_vocab = Vocab({v:k for k,v in dict(enumerate(characters,start=2)).items()},for_encoder=True)\n",
    "        self.transcripts_vocab = Vocab({v:k for k,v in dict(enumerate(transcripts,start=4)).items()})\n",
    "    \n",
    "            \n",
    "    def collate_fn(self, batch): \n",
    "        x_values     =  []\n",
    "        x_lengths    =  []\n",
    "        y_values_in  =  []\n",
    "        y_values_out =  []\n",
    "        \n",
    "        for item in batch:\n",
    "            \n",
    "            x_values.append([self.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "            y_values_in.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            y_values_out.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            \n",
    "        sorted_tuples = sorted(zip(x_values,y_values_in,y_values_out),key=lambda x:len(x[0]),reverse=True)\n",
    "        x_values     =  [l[0] for l in sorted_tuples]\n",
    "        y_values_in  =  [l[1] for l in sorted_tuples]\n",
    "        y_values_out =  [l[2] for l in sorted_tuples]\n",
    "        \n",
    "        max_x = len(x_values[0])\n",
    "        max_y = max(len(l) for l in y_values_in)\n",
    "            \n",
    "        for word_index in range(len(x_values)):\n",
    "            length_of_current_x = len(x_values[word_index])\n",
    "            length_of_current_y = len(y_values_in[word_index])\n",
    "\n",
    "            x_lengths.append(length_of_current_x)\n",
    "            y_values_out[word_index].append(3)\n",
    "            \n",
    "            for _ in range(max_x - length_of_current_x):\n",
    "                x_values[word_index].append(0)\n",
    "            for _ in range(max_y - length_of_current_y):\n",
    "                y_values_in[word_index].append(0)\n",
    "                y_values_out[word_index].append(0)\n",
    "            \n",
    "            y_values_in[word_index].insert(0,2)\n",
    "            \n",
    "        y_values_in_tensor = torch.tensor(y_values_in)\n",
    "        x_values_tensor     = torch.tensor(x_values)\n",
    "        y_values_out_tensor = torch.tensor(y_values_out)\n",
    "        \n",
    "        return x_values_tensor,x_lengths,y_values_in_tensor,y_values_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharactersDataset(data_path)\n",
    "\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices      = list(range(dataset_size))\n",
    "valid_size   = int(np.floor(0.2 * dataset_size))\n",
    "test_size    = valid_size + int(np.floor(0.05 * dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_indices, test_indeces,train_indices = indices[:valid_size], indices[valid_size:test_size],indices[test_size:]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indeces)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                           sampler=train_sampler,collate_fn = dataset.collate_fn)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=valid_sampler,collate_fn = dataset.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=test_sampler,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers = 1,dropout=0):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size,embed_dim,padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq,input_lengths,hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths,batch_first=True)\n",
    "        \n",
    "        outputs,hidden = self.LSTM(packed)\n",
    "        \n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers=1,dropout=0):\n",
    "        super(AttentionDecoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_size,embed_dim, padding_idx = 0)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout = (0 if n_layers == 1 else self.dropout),batch_first=True)\n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_step,last_hidden,encoder_outputs):\n",
    "        #encoder_outputs.shape (batch_size,encoder_seq_len,hidden_dim)\n",
    "        \n",
    "        #input_step (batch_size)\n",
    "        input_step = input_step.unsqueeze(1)\n",
    "        #(batch_size,1)\n",
    "        embedded = self.embedding(input_step)\n",
    "        #embedded(batch_size,1,hidden_dim)\n",
    "        \n",
    "        decoder_outputs,hidden = self.LSTM(embedded,last_hidden)\n",
    "        #(batch_size,1,hidden_dim)\n",
    "        #seq_len = 1 if we using teacher forcing\n",
    "        \n",
    "        at_weights = decoder_outputs.bmm(encoder_outputs.transpose(1,2))\n",
    "        #(batch_size,1,encoder_seq_len)\n",
    "        \n",
    "        at_weights = F.softmax(at_weights, dim=2)\n",
    "        #(batch_size,1,encoder_seq_len)\n",
    "\n",
    "        context = at_weights.bmm(encoder_outputs)\n",
    "        #(batch_size,1,encoder_seq_len) * (batch_size,encoder_seq_len,hidden_dim)\n",
    "        \n",
    "        decoder_outputs = decoder_outputs.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        #(batch_size,hidden_dim)    #we deleated seq_len,which is = 1\n",
    "        \n",
    "        concat_input = torch.cat((decoder_outputs,context),1)\n",
    "        #(batch_size,hidden_size*2)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        #(batch_size,hidden_size)\n",
    "        predictions = F.softmax(self.out(concat_output),dim=1)\n",
    "        #(batch_size,output_dim)\n",
    "        return predictions,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE        = 32\n",
    "HIDDEN_DIM        = 64\n",
    "INPUT_VOCAB_SIZE  = len(dataset.characters_vocab)\n",
    "OUTPUT_VOCAB_SIZE = len(dataset.transcripts_vocab)\n",
    "\n",
    "\n",
    "encoder = EncoderLSTM(BATCH_SIZE,HIDDEN_DIM,INPUT_VOCAB_SIZE).to(device)\n",
    "decoder = AttentionDecoderLSTM(BATCH_SIZE,HIDDEN_DIM,OUTPUT_VOCAB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device  = device\n",
    "    \n",
    "    \n",
    "    def forward(self,x,x_lengths,y,teacher_forcing_ratio = 1):\n",
    "        \n",
    "        encoder_outputs,last_hidden = self.encoder(x,x_lengths)\n",
    "       \n",
    "        \n",
    "        batch_size         = y.shape[0]\n",
    "        seq_len            = y.shape[1]\n",
    "        output_vocab_size  = self.decoder.output_size\n",
    "        predictions        = torch.zeros(batch_size,seq_len,output_vocab_size)\n",
    "        \n",
    "        input_token = y[:,0]\n",
    "        \n",
    "        for t in range(1,seq_len):\n",
    "            \n",
    "            decoder_output,last_hidden = self.decoder(input_token,last_hidden,encoder_outputs)\n",
    "            predictions[:,t-1] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = decoder_output.max(1)[1]\n",
    "            input_token = (y[:,t] if teacher_force else top1)\n",
    "        \n",
    "        decoder_output,last_hidden = self.decoder(input_token,last_hidden,encoder_outputs)\n",
    "        predictions[:,t] = decoder_output\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def predict(self,x,x_lengths):\n",
    "        \n",
    "        #batch_size = 1!\n",
    "        x.unsqueeze_(0)\n",
    "        encoder_outputs,hidden = self.encoder(x,x_lengths)\n",
    "        char_to_input = torch.LongTensor([2]).to(device)\n",
    "        preds = []\n",
    "        while True:\n",
    "            \n",
    "            predictions,hidden = self.decoder(char_to_input, hidden, encoder_outputs)\n",
    "           \n",
    "            index_of_next = torch.argmax(predictions,dim=1)\n",
    "            our_value = index_of_next.item()\n",
    "            if our_value == 3:\n",
    "                break\n",
    "            char_to_input = index_of_next\n",
    "            preds.append(our_value)\n",
    "            \n",
    "        return preds                    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = seq2seq(encoder,decoder,device)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,epoch,train_loss_list,teacher_forcing_ratio):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    teacher_forcing_ratio = 1.0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x = batch[0].to(device)\n",
    "        x_lengths = batch[1]\n",
    "        y_in = batch[2].to(device)\n",
    "        y_out = batch[3].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x,x_lengths, y_in,teacher_forcing_ratio)\n",
    "        #output dim (y_seq_len,batch_size,output_dim)\n",
    "        output = output.view(-1,output.shape[-1]).to(device)\n",
    "        y_out = y_out.view(-1)\n",
    "        loss = criterion(output, y_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion,validation_losses):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            x_lengths = batch[1]\n",
    "            y_in = batch[2].to(device)\n",
    "            y_out = batch[3].to(device)\n",
    "            \n",
    "            output = model(x,x_lengths,y_in,0) #turn off teacher forcing\n",
    "            \n",
    "            output = output.view(-1,output.shape[-1]).to(device)\n",
    "            y_out = y_out.view(-1)\n",
    "\n",
    "            loss = criterion(output, y_out)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            validation_losses.append(loss.item())\n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(epoch, train_losses, val_losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('epoch %s. | loss: %s' % (epoch, np.mean(train_losses[-100:])))\n",
    "    plt.plot(train_losses)\n",
    "    plt.subplot(132)\n",
    "    plt.title('epoch %s. | loss: %s' % (epoch, np.mean(val_losses[-100:])))\n",
    "    plt.plot(val_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    if epoch > 2:\n",
    "        teacher_forcing_ratio = 0.3\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,epoch,train_losses,teacher_forcing_ratio)\n",
    "    valid_loss = evaluate(model, validation_loader, criterion,validation_losses)\n",
    "    \n",
    "    \n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    plot(epoch,train_losses,validation_losses)\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}')                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'encoder_weights')\n",
    "torch.save(decoder.state_dict(),'decoder_weights')\n",
    "torch.save(model.state_dict(),'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('encoder_weights'))\n",
    "decoder.load_state_dict(torch.load('decoder_weights'))\n",
    "model = seq2seq(encoder,decoder,device)\n",
    "model.load_state_dict(torch.load('model_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model,batch,batch_size):\n",
    "    x_tokens = []\n",
    "    y_tokens_predicted = []\n",
    "    y_tokens_true = []\n",
    "    for i in range(batch_size):\n",
    "        x_tokens.append(''.join([dataset.characters_vocab.idx2token(k.item()) for k in batch[0][i] if k.item() != 0]))\n",
    "        y_pred = model.predict(batch[0][i].to(device),[batch[1][i]])\n",
    "        y_tokens_predicted.append(''.join([dataset.transcripts_vocab.idx2token(k) for k in y_pred]))\n",
    "        y_tokens_true.append(''.join([dataset.transcripts_vocab.idx2token(k.item()) for k in batch[2][i] if k.item() != 0 and k.item() != 2]))\n",
    "    for i in range(len(x_tokens)):\n",
    "        print('Input:',x_tokens[i])\n",
    "        print('Predicted_output:',y_tokens_predicted[i])\n",
    "        print('True output:',y_tokens_true[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    predict_batch(model,batch,32)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
