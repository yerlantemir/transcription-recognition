{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "root_path = 'data/transcriptions'\n",
    "data_path = os.path.join(root_path,'train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter,for_encoder=False, min_freq=None):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        if for_encoder:\n",
    "            self._token2idx = {\n",
    "                self.pad:self.pad_idx,\n",
    "                self.unk:self.unk_idx,\n",
    "            }\n",
    "        else:\n",
    "            self._token2idx = {\n",
    "                self.sos: self.sos_idx,\n",
    "                self.eos: self.eos_idx,\n",
    "                self.pad: self.pad_idx,\n",
    "                self.unk: self.unk_idx,\n",
    "            }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def sent2idx(self, sent):\n",
    "        return [self.token2idx(i) for i in sent]\n",
    "    \n",
    "    def idx2sent(self, idx):\n",
    "        return [self.idx2token(i) for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return '{}'.format(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharactersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file_path,transform = None):\n",
    "        self.file = pd.read_csv(csv_file_path,'r')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.characters_vocab = None\n",
    "        self.transcripts_vocab = None\n",
    "        self.non_needed_symbols = '\\'#$?\\\\_({)}-:\\\";!%.1234567890'\n",
    "        \n",
    "        self.make_dataset()\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.data[idx]['x']\n",
    "        y = self.data[idx]['y']\n",
    "        data = {'x':x,'y':y}\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        characters = set()\n",
    "        transcripts = set()\n",
    "        \n",
    "        for idx in range(len(self.file)):\n",
    "            item = str(self.file.iloc[idx][0]).split(',')\n",
    "            \n",
    "            x = item[1].strip()\n",
    "            for symbol in self.non_needed_symbols:\n",
    "                x = x.replace(symbol,'')\n",
    "            if x == '':\n",
    "                continue\n",
    "            y = item[2].replace(' ','')\n",
    "            self.data.append({'x':x,'y':y})\n",
    "            for character in x:\n",
    "                characters.add(character)\n",
    "            for transcript in y:\n",
    "                transcripts.add(transcript)\n",
    "        \n",
    "        self.characters_vocab = Vocab({v:k for k,v in dict(enumerate(characters,start=2)).items()},for_encoder=True)\n",
    "        self.transcripts_vocab = Vocab({v:k for k,v in dict(enumerate(transcripts,start=4)).items()})\n",
    "    \n",
    "            \n",
    "    def collate_fn(self, batch): \n",
    "        x_values     =  []\n",
    "        x_lengths    =  []\n",
    "        y_values_in  =  []\n",
    "        y_values_out =  []\n",
    "        \n",
    "        for item in batch:\n",
    "            \n",
    "            x_values.append([self.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "            y_values_in.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            y_values_out.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            \n",
    "        sorted_tuples = sorted(zip(x_values,y_values_in,y_values_out),key=lambda x:len(x[0]),reverse=True)\n",
    "        x_values     =  [l[0] for l in sorted_tuples]\n",
    "        y_values_in  =  [l[1] for l in sorted_tuples]\n",
    "        y_values_out =  [l[2] for l in sorted_tuples]\n",
    "        \n",
    "        max_x = len(x_values[0])\n",
    "        max_y = max(len(l) for l in y_values_in)\n",
    "            \n",
    "        for word_index in range(len(x_values)):\n",
    "            length_of_current_x = len(x_values[word_index])\n",
    "            length_of_current_y = len(y_values_in[word_index])\n",
    "\n",
    "            x_lengths.append(length_of_current_x)\n",
    "            y_values_out[word_index].append(3)\n",
    "            for _ in range(max_x - length_of_current_x):\n",
    "                x_values[word_index].append(0)\n",
    "            for _ in range(max_y - length_of_current_y):\n",
    "                y_values_in[word_index].append(0)\n",
    "                y_values_out[word_index].append(0)\n",
    "            \n",
    "            y_values_in[word_index].insert(0,2)\n",
    "            \n",
    "        y_values_in_tensor = torch.tensor(y_values_in)\n",
    "        x_values_tensor     = torch.tensor(x_values)\n",
    "        y_values_out_tensor = torch.tensor(y_values_out)\n",
    "        \n",
    "        return x_values_tensor,x_lengths,y_values_in_tensor,y_values_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = CharactersDataset(data_path)\n",
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True,num_workers=2,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                           sampler=train_sampler,collate_fn = dataset.collate_fn)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=valid_sampler,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers = 1,dropout=0):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size,embed_dim,padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq,input_lengths,hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths,batch_first=True)\n",
    "        \n",
    "        outputs,hidden = self.LSTM(packed)\n",
    "        \n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers=1,dropout=0):\n",
    "        super(AttentionDecoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_size,embed_dim, padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout = (0 if n_layers == 1 else self.dropout),batch_first=True)\n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_step,last_hidden,encoder_outputs):\n",
    "        #encoder_outputs.shape (batch_size,encoder_seq_len,hidden_dim)\n",
    "        \n",
    "        #input_step (batch_size)\n",
    "        input_step = input_step.unsqueeze(1)\n",
    "        #(batch_size,1)\n",
    "        embedded = self.embedding(input_step)\n",
    "        #embedded(batch_size,1,hidden_dim)\n",
    "        \n",
    "        decoder_outputs,hidden = self.LSTM(embedded,last_hidden)\n",
    "        #(batch_size,1,hidden_dim)\n",
    "        #seq_len = 1 if we using teacher forcing\n",
    "        \n",
    "        at_weights = decoder_outputs.bmm(encoder_outputs.transpose(1,2))\n",
    "        #(batch_size,1,encoder_seq_len)\n",
    "        \n",
    "        at_weights = F.softmax(at_weights, dim=2)\n",
    "        #(batch_size,1,encoder_seq_len)\n",
    "\n",
    "        context = at_weights.bmm(encoder_outputs)\n",
    "        #(batch_size,1,encoder_seq_len) * (batch_size,encoder_seq_len,hidden_dim)\n",
    "        \n",
    "        decoder_outputs = decoder_outputs.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        #(batch_size,hidden_dim)    #we deleated seq_len,which is = 1\n",
    "        \n",
    "        concat_input = torch.cat((decoder_outputs,context),1)\n",
    "        #(batch_size,hidden_size*2)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        #(batch_size,hidden_size)\n",
    "        predictions = F.softmax(self.out(concat_output),dim=1)\n",
    "        #(batch_size,output_dim)\n",
    "        return predictions,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "BATCH_SIZE        = 32\n",
    "HIDDEN_DIM        = 64\n",
    "INPUT_VOCAB_SIZE  = len(dataset.characters_vocab)\n",
    "OUTPUT_VOCAB_SIZE = len(dataset.transcripts_vocab)\n",
    "encoder = EncoderLSTM(BATCH_SIZE,HIDDEN_DIM,INPUT_VOCAB_SIZE).to(device)\n",
    "decoder = AttentionDecoderLSTM(BATCH_SIZE,HIDDEN_DIM,OUTPUT_VOCAB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device  = device\n",
    "    \n",
    "    \n",
    "    def forward(self,x,x_lengths,y,teacher_forcing_ratio = 1):\n",
    "        \n",
    "        encoder_outputs,last_hidden = self.encoder(x,x_lengths)\n",
    "       \n",
    "        \n",
    "        batch_size         = y.shape[0]\n",
    "        seq_len            = y.shape[1]\n",
    "        output_vocab_size  = self.decoder.output_size\n",
    "        outputs            = torch.zeros(batch_size,seq_len,output_vocab_size)\n",
    "        \n",
    "        input_token = y[:,0]\n",
    "        \n",
    "        for t in range(1,seq_len):\n",
    "            \n",
    "            decoder_output,last_hidden = self.decoder(input_token,last_hidden,encoder_outputs)\n",
    "            outputs[:,t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = decoder_output.max(1)[1]\n",
    "            input_token = (y[:,t] if teacher_force else top1)\n",
    "        \n",
    "        decoder_output,last_hidden = self.decoder(input_token,last_hidden,encoder_outputs)\n",
    "        outputs[:,t] = decoder_output\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    def predict(self,x,x_length):\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 62,044 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq(encoder,decoder,device)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,epoch,train_loss_list,teacher_forcing_ratio):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    teacher_forcing_ratio = 1.0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x = batch[0].to(device)\n",
    "        x_lengths = batch[1]\n",
    "        y_in = batch[2].to(device)\n",
    "        y_out = batch[3].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x,x_lengths, y_in,teacher_forcing_ratio)\n",
    "        #output dim (y_seq_len,batch_size,output_dim)\n",
    "        output = output.view(-1,output.shape[-1]).to(device)\n",
    "        y_out = y_out.view(-1)\n",
    "        loss = criterion(output, y_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            plot(epoch, i, train_loss_list)\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            x_lengths = batch[1]\n",
    "            y_in = batch[2].to(device)\n",
    "            y_out = batch[3].to(device)\n",
    "            \n",
    "            output = model(x,x_lengths,y_in,0) #turn off teacher forcing\n",
    "            \n",
    "            output = output.view(-1,output.shape[-1]).to(device)\n",
    "            y_out = y_out.view(-1)\n",
    "\n",
    "            loss = criterion(output, y_out)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "def plot(epoch, step, train_losses):\n",
    "    clear_output(True)\n",
    "    plt.title(f'Epochs {epoch}, step {step}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    kek = batch\n",
    "    break\n",
    "x = kek[0]\n",
    "x_lenghts = kek[1]\n",
    "y_in = kek[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.733 | Train PPL:  15.374\n",
      "\tTrain Loss: 3.152 | Train PPL:  23.383\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 30\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    if epoch > 10:\n",
    "        teacher_forcing_ratio = 0.5\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,epoch,train_losses,teacher_forcing_ratio)\n",
    "    valid_loss = evaluate(model, validation_loader, criterion)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tTrain Loss: {valid_loss:.3f} | Train PPL: {math.exp(valid_loss):7.3f}')                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'encoder_weights')\n",
    "torch.save(decoder.state_dict(),'decoder_weights')\n",
    "torch.save(model.state_dict(),'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('encoder_weights'))\n",
    "decoder.load_state_dict(torch.load('decoder_weights'))\n",
    "model = seq2seq(encoder,decoder,device)\n",
    "model.load_state_dict(torch.load('model_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in validation_loader:\n",
    "    kek = batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(kek[0].to(device),kek[1],kek[2].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([27,  9, 22,  5, 20,  4, 10, 16,  4,  4, 24,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3]),\n",
       " '\\n',\n",
       " tensor([ 2, 11,  9, 22,  5,  4, 22, 10, 17,  4, 25, 24,  0,  0,  0,  0,  0,  0,\n",
       "          0]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1].max(1)[1],'\\n',kek[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model,batch,batch_size):\n",
    "    x_tokens = []\n",
    "    y_tokens_predicted = []\n",
    "    y_tokens_true = []\n",
    "    for i in range(batch_size):\n",
    "        print(i)\n",
    "        x_tokens.append(''.join([dataset.characters_vocab.idx2token(k.item()) for k in batch[0][i] if k.item() != 0]))\n",
    "        y_pred = model.predict(batch[0][i].to(device),[batch[1][i]])\n",
    "        y_tokens_predicted.append(''.join([dataset.transcripts_vocab.idx2token(k) for k in y_pred]))\n",
    "        y_tokens_true.append(''.join([dataset.transcripts_vocab.idx2token(k.item()) for k in batch[2][i] if k.item() != 0 and k.item() != 2]))\n",
    "    for i in range(len(x_tokens)):\n",
    "        print('Input:',x_tokens[i])\n",
    "        print('Predicted_output:',y_tokens_predicted[i])\n",
    "        print('True output:',y_tokens_true[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ''\n",
    "out = ''\n",
    "for i in x: \n",
    "    inp += dataset.characters_vocab.idx2token(i.item())\n",
    "for i in y:\n",
    "    out += dataset.transcripts_vocab.idx2token(i.item())\n",
    "inp,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.argmax(predictions).unsqueeze_(0).unsqueeze_(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kek[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
