{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "root_path = 'data/transcriptions'\n",
    "data_path = os.path.join(root_path,'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter,for_encoder=False, min_freq=None):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        if for_encoder:\n",
    "            self._token2idx = {\n",
    "                self.pad:self.pad_idx,\n",
    "                self.unk:self.unk_idx,\n",
    "            }\n",
    "        else:\n",
    "            self._token2idx = {\n",
    "                self.sos: self.sos_idx,\n",
    "                self.eos: self.eos_idx,\n",
    "                self.pad: self.pad_idx,\n",
    "                self.unk: self.unk_idx,\n",
    "            }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def sent2idx(self, sent):\n",
    "        return [self.token2idx(i) for i in sent]\n",
    "    \n",
    "    def idx2sent(self, idx):\n",
    "        return [self.idx2token(i) for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return '{}'.format(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharactersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file_path,transform = None):\n",
    "        self.file = pd.read_csv(csv_file_path,'r')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.characters_vocab = None\n",
    "        self.transcripts_vocab = None\n",
    "        self.non_needed_symbols = '\\'#$?\\\\_({)}-:\\\";!%.1234567890'\n",
    "        \n",
    "        self.make_dataset()\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.data[idx]['x']\n",
    "        y = self.data[idx]['y']\n",
    "        data = {'x':x,'y':y}\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        characters = set()\n",
    "        transcripts = set()\n",
    "        for idx in range(len(self.file)):\n",
    "            item = str(self.file.iloc[idx][0]).split(',')\n",
    "            \n",
    "            x = item[1].strip()\n",
    "            for symbol in self.non_needed_symbols:\n",
    "                x = x.replace(symbol,'')\n",
    "            y = item[2].replace(' ','')\n",
    "            self.data.append({'x':x,'y':y})\n",
    "            for character in x:\n",
    "                characters.add(character)\n",
    "            for transcript in y:\n",
    "                transcripts.add(transcript)\n",
    "        \n",
    "        self.characters_vocab = Vocab({v:k for k,v in dict(enumerate(characters,start=2)).items()},for_encoder=True)\n",
    "        self.transcripts_vocab = Vocab({v:k for k,v in dict(enumerate(transcripts,start=4)).items()})\n",
    "        \n",
    "            \n",
    "    def collate_fn(self, batch): \n",
    "        x_values = []\n",
    "        y_values_in = []\n",
    "        x_lengths = []\n",
    "        y_lengths = []\n",
    "        for item in batch:\n",
    "            \n",
    "            x_values.append([self.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "            y_values_in.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "        \n",
    "        x_values = sorted(x_values,key=len,reverse=True)\n",
    "        y_values_in = sorted(y_values_in,key=len,reverse=True)\n",
    "        \n",
    "        max_x = len(x_values[0])\n",
    "        max_y = len(y_values_in[0])\n",
    "        \n",
    "        for word_index in range(len(x_values)):\n",
    "            \n",
    "            x_lengths.append(len(x_values[word_index]))\n",
    "            y_lengths.append(len(y_values_in[word_index]))\n",
    "            \n",
    "            for _ in range(max_x - len(x_values[word_index])):\n",
    "                x_values[word_index].append(0)\n",
    "            for _ in range(max_y - len(y_values_in[word_index])):\n",
    "                y_values_in[word_index].append(0)\n",
    "            \n",
    "            y_values_in[word_index].insert(0,2)\n",
    "            \n",
    "        x_values = torch.tensor(x_values)\n",
    "        y_values_in_tensor = torch.tensor(y_values_in)\n",
    "        \n",
    "        y_values_out = y_values_in        \n",
    "        for arr_index in range(len(y_values_out)):\n",
    "            y_values_out[arr_index] = y_values_out[arr_index][1:] + [3]\n",
    "        y_values_out_tensor = torch.tensor(y_values_out)\n",
    "        \n",
    "        return x_values,y_values_in_tensor,y_values_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = CharactersDataset(data_path)\n",
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True,num_workers=2,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                           sampler=train_sampler,collate_fn = dataset.collate_fn)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=valid_sampler,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers = 1,dropout=0):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size,embed_dim,padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq,hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        #packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths)\n",
    "        outputs,(hidden,cell) = self.LSTM(embedded)\n",
    "        #outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers=1,dropout=0):\n",
    "        super(DecoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_size,embed_dim, padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout = (0 if n_layers == 1 else self.dropout),batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_step,last_hidden, last_cell):\n",
    "        #input_step (1,batch_size)\n",
    "        input_step = input_step.unsqueeze(1)\n",
    "\n",
    "        embedded = self.embedding(input_step)\n",
    "        #embedded(1,batch_size,hidden_dim)\n",
    "\n",
    "        output,(hidden,cell) = self.LSTM(embedded,(last_hidden, last_cell))\n",
    "        #output(batch_size,seq_len,hidden_dim)\n",
    "        #seq_len = 1 if we using teacher forcing\n",
    "        output = output.squeeze(1) #(batch_size,hidden_dim)\n",
    "        \n",
    "        prediction = F.relu(self.out(output))\n",
    "        #prediction(batch_size,output_dim)\n",
    "        \n",
    "        return prediction,hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(128,256,len(dataset.characters_vocab),2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderLSTM(128,256,len(dataset.transcripts_vocab),2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden,cell = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,hidden,cell = decoder(y[:,0],hidden,cell)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self,x,y,teacher_forcing_ratio = 0.3):\n",
    "        batch_size = y.shape[0]\n",
    "        seq_len = y.shape[1]\n",
    "        outputs = torch.zeros(seq_len,batch_size,self.decoder.output_size).to(self.device)\n",
    "        \n",
    "            \n",
    "        hidden,cell = self.encoder(x)\n",
    "        input_token = y[:,0]\n",
    "        \n",
    "        for t in range(1,seq_len):\n",
    "            output,hidden,cell = self.decoder(input_token,hidden,cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input_token = (y[:,t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self,x):\n",
    "\n",
    "        #batch_size = 1!\n",
    "        hidden,cell = self.encoder(x)\n",
    "        out_input = torch.LongTensor([[2]]).to(device)\n",
    "        preds = []\n",
    "        while True:\n",
    "            output, hidden, cell = self.decoder(out_input, hidden, cell)\n",
    "            output = torch.argmax(output)\n",
    "            our_value = output.item()\n",
    "            if our_value == 3:\n",
    "                break\n",
    "            preds.append(our_value)\n",
    "            out_input = output.unsqueeze(0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq(encoder,decoder,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = x[0,:]\n",
    "inp.unsqueeze_(0)\n",
    "h,c = encoder(inp)\n",
    "out_input = torch.LongTensor([2]).to(device)\n",
    "preds = []\n",
    "while True:\n",
    "    output,h,c = decoder(out_input,h,c)\n",
    "    output = torch.argmax(output)\n",
    "    our_value = output.item()\n",
    "    if our_value == 3:\n",
    "        break\n",
    "    preds.append(our_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    kek = batch\n",
    "    break\n",
    "x = kek[0].to(device)\n",
    "y = kek[1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "input_token = y[:,0]\n",
    "hidden,cell = encoder(x)\n",
    "output,hidden,cell = decoder(input_token,hidden,cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1 = output.max(1)[1]\n",
    "top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): EncoderLSTM(\n",
       "    (embedding): Embedding(28, 128, padding_idx=0)\n",
       "    (LSTM): LSTM(128, 256, num_layers=2, batch_first=True)\n",
       "  )\n",
       "  (decoder): DecoderLSTM(\n",
       "    (embedding): Embedding(28, 128, padding_idx=0)\n",
       "    (LSTM): LSTM(128, 256, num_layers=2, batch_first=True)\n",
       "    (out): Linear(in_features=256, out_features=28, bias=True)\n",
       "    (softmax): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,857,564 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<sos>': 2, '<eos>': 3, '<pad>': 0, '<unk>': 1, 'L': 4, 'I': 5, 'J': 6, 'V': 7, 'O': 8, 'Y': 9, 'Z': 10, 'K': 11, 'R': 12, 'W': 13, 'G': 14, 'H': 15, 'B': 16, 'E': 17, 'N': 18, 'D': 19, 'T': 20, 'F': 21, 'A': 22, 'P': 23, 'M': 24, 'C': 25, 'S': 26, 'U': 27}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.transcripts_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,epoch,train_loss_list):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x = batch[0].to(device)\n",
    "        y_in = batch[1].to(device)\n",
    "        y_out = batch[2].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x, y_in)\n",
    "        #output dim (y_seq_len,batch_size,output_dim)\n",
    "        output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "        y_out = y_out.view(-1)\n",
    "        \n",
    "        loss = criterion(output, y_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            plot(epoch, i, train_loss_list)\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            y_in = batch[1].to(device)\n",
    "            y_out = batch[2].to(device)\n",
    "            \n",
    "            output = model(x,y_in,0) #turn off teacher forcing\n",
    "            \n",
    "            output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "            y_out = y_out.view(-1)\n",
    "\n",
    "            loss = criterion(output, y_out)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "def plot(epoch, step, train_losses):\n",
    "    clear_output()\n",
    "    plt.title(f'Epochs {epoch}, step {step}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.972 | Train PPL:  19.523\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,epoch,train_losses)\n",
    "    valid_loss = evaluate(model, validation_loader, criterion)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'encoder_weights')\n",
    "torch.save(decoder.state_dict(),'decoder_weights')\n",
    "torch.save(model.state_dict(),'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(len(dataset.characters_vocab),128).to(device)\n",
    "decoder = DecoderLSTM(128,len(dataset.transcripts_vocab)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('encoder_weights'))\n",
    "decoder.load_state_dict(torch.load('decoder_weights'))\n",
    "model = seq2seq(encoder,decoder,device)\n",
    "model.load_state_dict(torch.load('model_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in validation_loader:\n",
    "    kek = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = kek[0][0][1:].unsqueeze(0).to(device)\n",
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_token = ''\n",
    "for pred in y_pred:\n",
    "    output_token += dataset.transcripts_vocab.idx2token(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_token = ''\n",
    "for x_val in x.squeeze(0):\n",
    "    input_token += dataset.characters_vocab.idx2token(x_val.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token,output_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
