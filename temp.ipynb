{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "root_path = 'data/transcriptions'\n",
    "data_path = os.path.join(root_path,'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter,for_encoder=False, min_freq=None):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        if for_encoder:\n",
    "            self._token2idx = {\n",
    "                self.pad:self.pad_idx,\n",
    "                self.unk:self.unk_idx,\n",
    "            }\n",
    "        else:\n",
    "            self._token2idx = {\n",
    "                self.sos: self.sos_idx,\n",
    "                self.eos: self.eos_idx,\n",
    "                self.pad: self.pad_idx,\n",
    "                self.unk: self.unk_idx,\n",
    "            }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def sent2idx(self, sent):\n",
    "        return [self.token2idx(i) for i in sent]\n",
    "    \n",
    "    def idx2sent(self, idx):\n",
    "        return [self.idx2token(i) for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return '{}'.format(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharactersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file_path,transform = None):\n",
    "        self.file = pd.read_csv(csv_file_path,'r')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.characters_vocab = None\n",
    "        self.transcripts_vocab = None\n",
    "        self.non_needed_symbols = '\\'#$?\\\\_({)}-:\\\";!%.1234567890'\n",
    "        \n",
    "        self.make_dataset()\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.data[idx]['x']\n",
    "        y = self.data[idx]['y']\n",
    "        data = {'x':x,'y':y}\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        characters = set()\n",
    "        transcripts = set()\n",
    "        \n",
    "        for idx in range(len(self.file)):\n",
    "            item = str(self.file.iloc[idx][0]).split(',')\n",
    "            \n",
    "            x = item[1].strip()\n",
    "            for symbol in self.non_needed_symbols:\n",
    "                x = x.replace(symbol,'')\n",
    "            if x == '':\n",
    "                continue\n",
    "            y = item[2].replace(' ','')\n",
    "            self.data.append({'x':x,'y':y})\n",
    "            for character in x:\n",
    "                characters.add(character)\n",
    "            for transcript in y:\n",
    "                transcripts.add(transcript)\n",
    "        \n",
    "        self.characters_vocab = Vocab({v:k for k,v in dict(enumerate(characters,start=2)).items()},for_encoder=True)\n",
    "        self.transcripts_vocab = Vocab({v:k for k,v in dict(enumerate(transcripts,start=4)).items()})\n",
    "    \n",
    "            \n",
    "    def collate_fn(self, batch): \n",
    "        x_values     =  []\n",
    "        x_lengths    =  []\n",
    "        y_values_in  =  []\n",
    "        y_values_out =  []\n",
    "        \n",
    "        for item in batch:\n",
    "            \n",
    "            x_values.append([self.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "            y_values_in.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            y_values_out.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            \n",
    "        sorted_tuples = sorted(zip(x_values,y_values_in,y_values_out),key=lambda x:len(x[0]),reverse=True)\n",
    "        x_values     =  [l[0] for l in sorted_tuples]\n",
    "        y_values_in  =  [l[1] for l in sorted_tuples]\n",
    "        y_values_out =  [l[2] for l in sorted_tuples]\n",
    "        \n",
    "        max_x = len(x_values[0])\n",
    "        max_y = max(len(l) for l in y_values_in)\n",
    "            \n",
    "        for word_index in range(len(x_values)):\n",
    "            length_of_current_x = len(x_values[word_index])\n",
    "            length_of_current_y = len(y_values_in[word_index])\n",
    "\n",
    "            x_lengths.append(length_of_current_x)\n",
    "            \n",
    "            for _ in range(1 +  max_x - length_of_current_x):\n",
    "                x_values[word_index].append(0)\n",
    "            for _ in range(1+ max_y - length_of_current_y):\n",
    "                y_values_in[word_index].append(0)\n",
    "            \n",
    "            y_values_in[word_index].insert(0,2)\n",
    "            \n",
    "        y_values_in_tensor = torch.tensor(y_values_in)\n",
    "        \n",
    "        y_values_out = y_values_in        \n",
    "        for arr_index in range(len(y_values_out)):\n",
    "            index_of_first_zero = y_values_out[arr_index].index(0)\n",
    "            y_values_out[arr_index][index_of_first_zero] = 3\n",
    "            y_values_out[arr_index] = y_values_out[arr_index][1:]+[0]\n",
    "\n",
    "        x_values_tensor     = torch.tensor(x_values)\n",
    "        y_values_out_tensor = torch.tensor(y_values_out)\n",
    "        \n",
    "        return x_values_tensor,x_lengths,y_values_in_tensor,y_values_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = CharactersDataset(data_path)\n",
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True,num_workers=2,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                           sampler=train_sampler,collate_fn = dataset.collate_fn)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=valid_sampler,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers = 1,dropout=0):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size,embed_dim,padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq,input_lengths,hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths,batch_first=True)\n",
    "        \n",
    "        outputs,hidden = self.LSTM(packed)\n",
    "        \n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers=1,dropout=0):\n",
    "        super(DecoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_size,embed_dim, padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout = (0 if n_layers == 1 else self.dropout),batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_step,last_hidden):\n",
    "        #input_step (batch_size,seq_len)\n",
    "        embedded = self.embedding(input_step)\n",
    "        #embedded(batch_size,seq_len,hidden_dim)\n",
    "\n",
    "        output,hidden = self.LSTM(embedded,last_hidden)\n",
    "        #output(batch_size,seq_len,hidden_dim)\n",
    "        #seq_len = 1 if we using teacher forcing\n",
    "        #output = output.squeeze(1) #(batch_size,hidden_dim) for teacher forcing\n",
    "        \n",
    "        prediction = self.out(output)\n",
    "        #prediction(seq_len,batch_size,output_dim) if no teacher_forcing\n",
    "        \n",
    "        return prediction,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(32,64,len(dataset.characters_vocab)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderLSTM(32,64,len(dataset.transcripts_vocab)).to(device)\n",
    "len(dataset.characters_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def forward(self,x,x_lengths,y,teacher_forcing_ratio = 0.3):\n",
    "        \n",
    "        encoder_output,hidden = self.encoder(x,x_lengths)\n",
    "        decoder_outputs,hidden = self.decoder(y,hidden)\n",
    "        \n",
    "       \n",
    "        #hidden = self.encoder(x)\n",
    "        #input_token = y[:,0]\n",
    "        \n",
    "        #for t in range(1,seq_len):\n",
    "         #   output,hidden = self.decoder(input_token,hidden)\n",
    "          #  outputs[t] = output\n",
    "           # teacher_force = random.random() < teacher_forcing_ratio\n",
    "            #top1 = output.max(1)[1]\n",
    "           # input_token = (y[:,t] if teacher_force else top1)\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def predict(self,x,x_lengths):\n",
    "        \n",
    "        #batch_size = 1!\n",
    "        x.unsqueeze_(0)\n",
    "        encoder_outputs,hidden = self.encoder(x,x_lengths)\n",
    "        char_to_input = torch.LongTensor([[2]]).to(device)\n",
    "        preds = []\n",
    "        while True:\n",
    "            predictions, hidden = self.decoder(char_to_input, hidden)\n",
    "            index_of_next = torch.argmax(predictions)\n",
    "            our_value = index_of_next.item()\n",
    "            if our_value == 3:\n",
    "                break\n",
    "            preds.append(our_value)\n",
    "            char_to_input = index_of_next.unsqueeze_(0).unsqueeze_(0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq(encoder,decoder,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): EncoderLSTM(\n",
       "    (embedding): Embedding(28, 32, padding_idx=0)\n",
       "    (LSTM): LSTM(32, 64, batch_first=True)\n",
       "  )\n",
       "  (decoder): DecoderLSTM(\n",
       "    (embedding): Embedding(28, 32, padding_idx=0)\n",
       "    (LSTM): LSTM(32, 64, batch_first=True)\n",
       "    (out): Linear(in_features=64, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 53,788 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,epoch,train_loss_list):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x = batch[0].to(device)\n",
    "        x_lengths = batch[1]\n",
    "        y_in = batch[2].to(device)\n",
    "        y_out = batch[3].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x,x_lengths, y_in)\n",
    "        #output dim (y_seq_len,batch_size,output_dim)\n",
    "        output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "        y_out = y_out.view(-1)\n",
    "        \n",
    "        loss = criterion(output, y_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            plot(epoch, i, train_loss_list)\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            x_lengths = batch[1]\n",
    "            y_in = batch[2].to(device)\n",
    "            y_out = batch[3].to(device)\n",
    "            \n",
    "            output = model(x,x_lengths,y_in,0) #turn off teacher forcing\n",
    "            \n",
    "            output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "            y_out = y_out.view(-1)\n",
    "\n",
    "            loss = criterion(output, y_out)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "def plot(epoch, step, train_losses):\n",
    "    clear_output(True)\n",
    "    plt.title(f'Epochs {epoch}, step {step}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,epoch,train_losses)\n",
    "    valid_loss = evaluate(model, validation_loader, criterion)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'encoder_weights')\n",
    "torch.save(decoder.state_dict(),'decoder_weights')\n",
    "torch.save(model.state_dict(),'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('encoder_weights'))\n",
    "decoder.load_state_dict(torch.load('decoder_weights'))\n",
    "model = seq2seq(encoder,decoder,device)\n",
    "model.load_state_dict(torch.load('model_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in validation_loader:\n",
    "    kek = batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 21, 17, 21, 20, 21,  6, 21,  8,  8,  4,  0,  0,  0])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kek[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model,batch,batch_size):\n",
    "    x_tokens = []\n",
    "    y_tokens_predicted = []\n",
    "    y_tokens_true = []\n",
    "    for i in range(batch_size):\n",
    "        x_tokens.append(''.join([dataset.characters_vocab.idx2token(k.item()) for k in batch[0][i] if k.item() != 0]))\n",
    "        y_pred = model.predict(batch[0][i].to(device),[batch[1][i]])\n",
    "        y_tokens_predicted.append(''.join([dataset.transcripts_vocab.idx2token(k) for k in y_pred]))\n",
    "        y_tokens_true.append(''.join([dataset.transcripts_vocab.idx2token(k.item()) for k in batch[2][i] if k.item() != 0 and k.item() != 2]))\n",
    "    for i in range(len(x_tokens)):\n",
    "        print('Input:',x_tokens[i])\n",
    "        print('Predicted_output:',y_tokens_predicted[i])\n",
    "        print('True output:',y_tokens_true[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CATAWBA<pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<sos>KAHTAOBAH<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>')"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = ''\n",
    "out = ''\n",
    "for i in x: \n",
    "    inp += dataset.characters_vocab.idx2token(i.item())\n",
    "for i in y:\n",
    "    out += dataset.transcripts_vocab.idx2token(i.item())\n",
    "inp,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kek[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-460-2430716c7d3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "x_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
