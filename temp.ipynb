{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "root_path = 'data/transcriptions'\n",
    "data_path = os.path.join(root_path,'train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter,for_encoder=False, min_freq=None):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        if for_encoder:\n",
    "            self._token2idx = {\n",
    "                self.pad:self.pad_idx,\n",
    "                self.unk:self.unk_idx,\n",
    "            }\n",
    "        else:\n",
    "            self._token2idx = {\n",
    "                self.sos: self.sos_idx,\n",
    "                self.eos: self.eos_idx,\n",
    "                self.pad: self.pad_idx,\n",
    "                self.unk: self.unk_idx,\n",
    "            }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def sent2idx(self, sent):\n",
    "        return [self.token2idx(i) for i in sent]\n",
    "    \n",
    "    def idx2sent(self, idx):\n",
    "        return [self.idx2token(i) for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return '{}'.format(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharactersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file_path,transform = None):\n",
    "        self.file = pd.read_csv(csv_file_path,'r')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.characters_vocab = None\n",
    "        self.transcripts_vocab = None\n",
    "        self.non_needed_symbols = '\\'#$?\\\\_({)}-:\\\";!%.1234567890'\n",
    "        \n",
    "        self.make_dataset()\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.data[idx]['x']\n",
    "        y = self.data[idx]['y']\n",
    "        data = {'x':x,'y':y}\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        characters = set()\n",
    "        transcripts = set()\n",
    "        \n",
    "        for idx in range(len(self.file)):\n",
    "            item = str(self.file.iloc[idx][0]).split(',')\n",
    "            \n",
    "            x = item[1].strip()\n",
    "            for symbol in self.non_needed_symbols:\n",
    "                x = x.replace(symbol,'')\n",
    "            if x == '':\n",
    "                continue\n",
    "            y = item[2].replace(' ','')\n",
    "            self.data.append({'x':x,'y':y})\n",
    "            for character in x:\n",
    "                characters.add(character)\n",
    "            for transcript in y:\n",
    "                transcripts.add(transcript)\n",
    "        \n",
    "        self.characters_vocab = Vocab({v:k for k,v in dict(enumerate(characters,start=2)).items()},for_encoder=True)\n",
    "        self.transcripts_vocab = Vocab({v:k for k,v in dict(enumerate(transcripts,start=4)).items()})\n",
    "    \n",
    "            \n",
    "    def collate_fn(self, batch): \n",
    "        x_values     =  []\n",
    "        x_lengths    =  []\n",
    "        y_values_in  =  []\n",
    "        y_values_out =  []\n",
    "        \n",
    "        for item in batch:\n",
    "            \n",
    "            x_values.append([self.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "            y_values_in.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            y_values_out.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "            \n",
    "        sorted_tuples = sorted(zip(x_values,y_values_in,y_values_out),key=lambda x:len(x[0]),reverse=True)\n",
    "        x_values     =  [l[0] for l in sorted_tuples]\n",
    "        y_values_in  =  [l[1] for l in sorted_tuples]\n",
    "        y_values_out =  [l[2] for l in sorted_tuples]\n",
    "        \n",
    "        max_x = len(x_values[0])\n",
    "        max_y = max(len(l) for l in y_values_in)\n",
    "            \n",
    "        for word_index in range(len(x_values)):\n",
    "            length_of_current_x = len(x_values[word_index])\n",
    "            length_of_current_y = len(y_values_in[word_index])\n",
    "\n",
    "            x_lengths.append(length_of_current_x)\n",
    "            y_values_out[word_index].append(3)\n",
    "            for _ in range(max_x - length_of_current_x):\n",
    "                x_values[word_index].append(0)\n",
    "            for _ in range(max_y - length_of_current_y):\n",
    "                y_values_in[word_index].append(0)\n",
    "                y_values_out[word_index].append(0)\n",
    "            \n",
    "            y_values_in[word_index].insert(0,2)\n",
    "            \n",
    "        y_values_in_tensor = torch.tensor(y_values_in)\n",
    "        x_values_tensor     = torch.tensor(x_values)\n",
    "        y_values_out_tensor = torch.tensor(y_values_out)\n",
    "        \n",
    "        return x_values_tensor,x_lengths,y_values_in_tensor,y_values_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = CharactersDataset(data_path)\n",
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True,num_workers=2,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                           sampler=train_sampler,collate_fn = dataset.collate_fn)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=valid_sampler,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers = 1,dropout=0):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size,embed_dim,padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq,input_lengths,hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths,batch_first=True)\n",
    "        \n",
    "        outputs,hidden = self.LSTM(packed)\n",
    "        \n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers=1,dropout=0):\n",
    "        super(AttentionDecoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_size,embed_dim, padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout = (0 if n_layers == 1 else self.dropout),batch_first=True)\n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_step,last_hidden,encoder_outputs):\n",
    "        #encoder_outputs.shape (batch_size,encoder_seq_len,hidden_dim)\n",
    "        \n",
    "        #input_step (batch_size)\n",
    "        input_step = input_step.unsqueeze(1)\n",
    "        #(batch_size,1)\n",
    "        embedded = self.embedding(input_step)\n",
    "        #embedded(batch_size,1,hidden_dim)\n",
    "        \n",
    "        decoder_outputs,hidden = self.LSTM(embedded,last_hidden)\n",
    "        #(batch_size,1,hidden_dim)\n",
    "        #seq_len = 1 if we using teacher forcing\n",
    "        \n",
    "        at_weights = decoder_outputs.bmm(encoder_outputs.transpose(1,2))\n",
    "        #(batch_size,1,encoder_seq_len)\n",
    "        \n",
    "        at_weights = F.softmax(at_weights, dim=2)\n",
    "        #(batch_size,1,encoder_seq_len)\n",
    "\n",
    "        context = at_weights.bmm(encoder_outputs)\n",
    "        #(batch_size,1,encoder_seq_len) * (batch_size,encoder_seq_len,hidden_dim)\n",
    "        \n",
    "        decoder_outputs = decoder_outputs.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        #(batch_size,hidden_dim)    #we deleated seq_len,which is = 1\n",
    "        \n",
    "        concat_input = torch.cat((decoder_outputs,context),1)\n",
    "        #(batch_size,hidden_size*2)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        #(batch_size,hidden_size)\n",
    "        predictions = self.out(concat_output)\n",
    "        #(batch_size,output_dim)\n",
    "        return predictions,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "BATCH_SIZE        = 32\n",
    "HIDDEN_DIM        = 64\n",
    "INPUT_VOCAB_SIZE  = len(dataset.characters_vocab)\n",
    "OUTPUT_VOCAB_SIZE = len(dataset.transcripts_vocab)\n",
    "encoder = EncoderLSTM(BATCH_SIZE,HIDDEN_DIM,INPUT_VOCAB_SIZE).to(device)\n",
    "decoder = AttentionDecoderLSTM(BATCH_SIZE,HIDDEN_DIM,OUTPUT_VOCAB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device  = device\n",
    "    \n",
    "    \n",
    "    def forward(self,x,x_lengths,y,teacher_forcing_ratio = 1):\n",
    "        \n",
    "        encoder_outputs,last_hidden = self.encoder(x,x_lengths)\n",
    "       \n",
    "        \n",
    "        batch_size         = y.shape[0]\n",
    "        seq_len            = y.shape[1]\n",
    "        output_vocab_size  = self.decoder.output_size\n",
    "        outputs            = torch.zeros(batch_size,seq_len,output_vocab_size)\n",
    "        \n",
    "        input_token = y[:,0]\n",
    "        \n",
    "        for t in range(1,seq_len):\n",
    "            \n",
    "            decoder_output,last_hidden = self.decoder(input_token,last_hidden,encoder_outputs)\n",
    "            outputs[:,t-1] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = decoder_output.max(1)[1]\n",
    "            input_token = (y[:,t] if teacher_force else top1)\n",
    "        \n",
    "        decoder_output,last_hidden = self.decoder(input_token,last_hidden,encoder_outputs)\n",
    "        outputs[:,t] = decoder_output\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    def predict(self,x,x_lengths):\n",
    "        \n",
    "        #batch_size = 1!\n",
    "        x.unsqueeze_(0)\n",
    "        encoder_outputs,hidden = self.encoder(x,x_lengths)\n",
    "        char_to_input = torch.LongTensor([2]).to(device)\n",
    "        preds = []\n",
    "        while True:\n",
    "            \n",
    "            predictions,hidden = self.decoder(char_to_input, hidden, encoder_outputs)\n",
    "           \n",
    "            index_of_next = torch.argmax(predictions,dim=1)\n",
    "            our_value = index_of_next.item()\n",
    "            if our_value == 3:\n",
    "                break\n",
    "            char_to_input = index_of_next\n",
    "            preds.append(our_value)\n",
    "            \n",
    "        return preds                    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 62,044 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq(encoder,decoder,device)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('encoder_weights'))\n",
    "decoder.load_state_dict(torch.load('decoder_weights'))\n",
    "model = seq2seq(encoder,decoder,device)\n",
    "model.load_state_dict(torch.load('model_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    kek = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model(kek[0].to(device),kek[1],kek[2].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kek[2].to(device)\n",
    "predicted.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predicted.view(-1,predicted.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted,kek[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,epoch,train_loss_list,teacher_forcing_ratio):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    teacher_forcing_ratio = 1.0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x = batch[0].to(device)\n",
    "        x_lengths = batch[1]\n",
    "        y_in = batch[2].to(device)\n",
    "        y_out = batch[3].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x,x_lengths, y_in,teacher_forcing_ratio)\n",
    "        #output dim (y_seq_len,batch_size,output_dim)\n",
    "        output = output.view(-1,output.shape[-1]).to(device)\n",
    "        y_out = y_out.view(-1)\n",
    "        loss = criterion(output, y_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            plot(epoch, i, train_loss_list)\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            x_lengths = batch[1]\n",
    "            y_in = batch[2].to(device)\n",
    "            y_out = batch[3].to(device)\n",
    "            \n",
    "            output = model(x,x_lengths,y_in,0) #turn off teacher forcing\n",
    "            \n",
    "            output = output.view(-1,output.shape[-1]).to(device)\n",
    "            y_out = y_out.view(-1)\n",
    "\n",
    "            loss = criterion(output, y_out)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "def plot(epoch, step, train_losses):\n",
    "    clear_output(True)\n",
    "    plt.title(f'Epochs {epoch}, step {step}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    kek = batch\n",
    "    break\n",
    "x = kek[0]\n",
    "x_lenghts = kek[1]\n",
    "y_in = kek[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\tValid Loss: 1.421 | Valid PPL:   4.141\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    if epoch > 2:\n",
    "        teacher_forcing_ratio = 0.3\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,epoch,train_losses,teacher_forcing_ratio)\n",
    "    valid_loss = evaluate(model, validation_loader, criterion)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}')                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'encoder_weights')\n",
    "torch.save(decoder.state_dict(),'decoder_weights')\n",
    "torch.save(model.state_dict(),'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in validation_loader:\n",
    "    kek = batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model,batch,batch_size):\n",
    "    x_tokens = []\n",
    "    y_tokens_predicted = []\n",
    "    y_tokens_true = []\n",
    "    for i in range(batch_size):\n",
    "        x_tokens.append(''.join([dataset.characters_vocab.idx2token(k.item()) for k in batch[0][i] if k.item() != 0]))\n",
    "        y_pred = model.predict(batch[0][i].to(device),[batch[1][i]])\n",
    "        y_tokens_predicted.append(''.join([dataset.transcripts_vocab.idx2token(k) for k in y_pred]))\n",
    "        y_tokens_true.append(''.join([dataset.transcripts_vocab.idx2token(k.item()) for k in batch[2][i] if k.item() != 0 and k.item() != 2]))\n",
    "    for i in range(len(x_tokens)):\n",
    "        print('Input:',x_tokens[i])\n",
    "        print('Predicted_output:',y_tokens_predicted[i])\n",
    "        print('True output:',y_tokens_true[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(kek[0][0].to(device),[kek[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 22, 20, 18, 13, 4, 24, 26, 27, 24, 15, 4, 18, 9, 24, 27, 24, 7],\n",
       " tensor([ 2,  8, 22, 20, 18, 13,  4, 24, 26, 27, 24, 15,  4, 18,  9, 24, 27, 24,\n",
       "          7]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted,kek[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ANTIREFORMERS\n",
      "Predicted_output: AENTEREHFERMERZ\n",
      "True output: AENTIYRIHFAORMERZ \n",
      "\n",
      "\n",
      "Input: WEISENBERGER\n",
      "Predicted_output: WAYSAHNBERGER\n",
      "True output: WAYZAHNBERGER \n",
      "\n",
      "\n",
      "Input: JAGODZINSKI\n",
      "Predicted_output: YAHGAADZIHNSKIY\n",
      "True output: YAHGAHJHIHNSKIY \n",
      "\n",
      "\n",
      "Input: QUEENFISHS\n",
      "Predicted_output: KWIYNFIHSHS\n",
      "True output: KWIYNFIHSHIHZ \n",
      "\n",
      "\n",
      "Input: DISPLACED\n",
      "Predicted_output: DIHSPLEYST\n",
      "True output: DIHSPLEYST \n",
      "\n",
      "\n",
      "Input: FEDDERSEN\n",
      "Predicted_output: FEHDERSAHN\n",
      "True output: FEHDERSAHN \n",
      "\n",
      "\n",
      "Input: ALEXANDRE\n",
      "Predicted_output: AELAHKSAENDREY\n",
      "True output: AELIHKSAANDER \n",
      "\n",
      "\n",
      "Input: GRIEVANCE\n",
      "Predicted_output: GRAYVAHNS\n",
      "True output: GRIYVAHNS \n",
      "\n",
      "\n",
      "Input: LATULIPPE\n",
      "Predicted_output: LAETAHLIHP\n",
      "True output: LAATUWLIYPIY \n",
      "\n",
      "\n",
      "Input: GREETHAM\n",
      "Predicted_output: GRIYTHAHM\n",
      "True output: GRIYTHAHM \n",
      "\n",
      "\n",
      "Input: PATRICKS\n",
      "Predicted_output: PAETRIHKS\n",
      "True output: PAETRIHKS \n",
      "\n",
      "\n",
      "Input: GUNDLACH\n",
      "Predicted_output: GAHNDLAEK\n",
      "True output: GAHNDLAHK \n",
      "\n",
      "\n",
      "Input: CLARITIN\n",
      "Predicted_output: KLAARIHTIHN\n",
      "True output: KLEHRIHTIHN \n",
      "\n",
      "\n",
      "Input: REMPFER\n",
      "Predicted_output: REHMPFER\n",
      "True output: REHMPFER \n",
      "\n",
      "\n",
      "Input: ELECTRA\n",
      "Predicted_output: EHLEHKTRAH\n",
      "True output: IHLEHKTRAH \n",
      "\n",
      "\n",
      "Input: MARLTON\n",
      "Predicted_output: MAARLTAHN\n",
      "True output: MAARLTAHN \n",
      "\n",
      "\n",
      "Input: BOOZIER\n",
      "Predicted_output: BUWZIYER\n",
      "True output: BUWZIYER \n",
      "\n",
      "\n",
      "Input: KASBROL\n",
      "Predicted_output: KAESBRAHL\n",
      "True output: KAEZBRAOL \n",
      "\n",
      "\n",
      "Input: INSLEY\n",
      "Predicted_output: IHNSLIY\n",
      "True output: IHNSLIY \n",
      "\n",
      "\n",
      "Input: MONCUS\n",
      "Predicted_output: MAANKAHS\n",
      "True output: MAANKIHS \n",
      "\n",
      "\n",
      "Input: BEMBRY\n",
      "Predicted_output: BEHMBRIY\n",
      "True output: BEHMBRIY \n",
      "\n",
      "\n",
      "Input: PESETA\n",
      "Predicted_output: PEHSEHTAH\n",
      "True output: PAHSEYTAH \n",
      "\n",
      "\n",
      "Input: ASBELL\n",
      "Predicted_output: AESBEHL\n",
      "True output: AESBEHL \n",
      "\n",
      "\n",
      "Input: ELMER\n",
      "Predicted_output: EHLMER\n",
      "True output: EHLMER \n",
      "\n",
      "\n",
      "Input: JULIO\n",
      "Predicted_output: YUWLIYOW\n",
      "True output: JHUWLIYOW \n",
      "\n",
      "\n",
      "Input: GIERE\n",
      "Predicted_output: JHIYR\n",
      "True output: JHIHR \n",
      "\n",
      "\n",
      "Input: GOODS\n",
      "Predicted_output: GUWDZ\n",
      "True output: GUHDZ \n",
      "\n",
      "\n",
      "Input: OFFEN\n",
      "Predicted_output: AOFAHN\n",
      "True output: AOFAHN \n",
      "\n",
      "\n",
      "Input: KILL\n",
      "Predicted_output: KIHL\n",
      "True output: KIHL \n",
      "\n",
      "\n",
      "Input: ITAR\n",
      "Predicted_output: AYTAAR\n",
      "True output: IYTAAR \n",
      "\n",
      "\n",
      "Input: SALM\n",
      "Predicted_output: SAELM\n",
      "True output: SAAM \n",
      "\n",
      "\n",
      "Input: EULA\n",
      "Predicted_output: YUWLAH\n",
      "True output: YUWLAH \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.argmax(predictions).unsqueeze_(0).unsqueeze_(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kek[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
