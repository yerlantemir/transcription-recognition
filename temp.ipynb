{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "root_path = 'data/transcriptions'\n",
    "data_path = os.path.join(root_path,'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter,for_encoder=False, min_freq=None):\n",
    "        self.sos = \"<sos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        if for_encoder:\n",
    "            self._token2idx = {\n",
    "                self.pad:self.pad_idx,\n",
    "                self.unk:self.unk_idx,\n",
    "            }\n",
    "        else:\n",
    "            self._token2idx = {\n",
    "                self.sos: self.sos_idx,\n",
    "                self.eos: self.eos_idx,\n",
    "                self.pad: self.pad_idx,\n",
    "                self.unk: self.unk_idx,\n",
    "            }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def sent2idx(self, sent):\n",
    "        return [self.token2idx(i) for i in sent]\n",
    "    \n",
    "    def idx2sent(self, idx):\n",
    "        return [self.idx2token(i) for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return '{}'.format(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharactersDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file_path,transform = None):\n",
    "        self.file = pd.read_csv(csv_file_path,'r')\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.characters_vocab = None\n",
    "        self.transcripts_vocab = None\n",
    "        self.non_needed_symbols = '\\'#$?\\\\_({)}-:\\\";!%.1234567890'\n",
    "        \n",
    "        self.make_dataset()\n",
    "       \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.data[idx]['x']\n",
    "        y = self.data[idx]['y']\n",
    "        data = {'x':x,'y':y}\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        characters = set()\n",
    "        transcripts = set()\n",
    "        for idx in range(len(self.file)):\n",
    "            item = str(self.file.iloc[idx][0]).split(',')\n",
    "            \n",
    "            x = item[1].strip()\n",
    "            for symbol in self.non_needed_symbols:\n",
    "                x = x.replace(symbol,'')\n",
    "            y = item[2].replace(' ','')\n",
    "            self.data.append({'x':x,'y':y})\n",
    "            for character in x:\n",
    "                characters.add(character)\n",
    "            for transcript in y:\n",
    "                transcripts.add(transcript)\n",
    "        \n",
    "        self.characters_vocab = Vocab({v:k for k,v in dict(enumerate(characters,start=2)).items()},for_encoder=True)\n",
    "        self.transcripts_vocab = Vocab({v:k for k,v in dict(enumerate(transcripts,start=4)).items()})\n",
    "    \n",
    "            \n",
    "    def collate_fn(self, batch): \n",
    "        x_values = []\n",
    "        y_values_in = []\n",
    "        x_lengths = []\n",
    "        y_lengths = []\n",
    "        for item in batch:\n",
    "            \n",
    "            x_values.append([self.characters_vocab.token2idx(ch) for ch in item['x']])\n",
    "            y_values_in.append([self.transcripts_vocab.token2idx(tr) for tr in item['y']])\n",
    "        \n",
    "        \n",
    "        max_x = len(max(x_values,key=len))\n",
    "        max_y = len(max(y_values_in,key=len))\n",
    "        \n",
    "        for word_index in range(len(x_values)):\n",
    "            \n",
    "            x_lengths.append(len(x_values[word_index]))\n",
    "            y_lengths.append(len(y_values_in[word_index]))\n",
    "            \n",
    "            for _ in range(1+ max_x - len(x_values[word_index])):\n",
    "                x_values[word_index].append(0)\n",
    "            for _ in range(1+ max_y - len(y_values_in[word_index])):\n",
    "                y_values_in[word_index].append(0)\n",
    "            \n",
    "            y_values_in[word_index].insert(0,2)\n",
    "            \n",
    "        x_values = torch.tensor(x_values)\n",
    "        y_values_in_tensor = torch.tensor(y_values_in)\n",
    "        \n",
    "        y_values_out = y_values_in        \n",
    "        for arr_index in range(len(y_values_out)):\n",
    "            index_of_first_zero = y_values_out[arr_index].index(0)\n",
    "            y_values_out[arr_index][index_of_first_zero] = 3\n",
    "            y_values_out[arr_index] = y_values_out[arr_index][1:]+[0]\n",
    "            \n",
    "        y_values_out_tensor = torch.tensor(y_values_out)\n",
    "        \n",
    "        return x_values,y_values_in_tensor,y_values_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = CharactersDataset(data_path)\n",
    "dataloader = DataLoader(dataset,batch_size=32,shuffle=True,num_workers=2,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kek in dataloader:\n",
    "    kek = kek\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n",
    "                                           sampler=train_sampler,collate_fn = dataset.collate_fn)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                                sampler=valid_sampler,collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers = 1,dropout=0):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size,embed_dim,padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout),batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq,hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        #packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths)\n",
    "        outputs,hidden = self.LSTM(embedded)\n",
    "        #outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_size,output_size,n_layers=1,dropout=0):\n",
    "        super(DecoderLSTM,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_size,embed_dim, padding_idx = 0)\n",
    "        self.LSTM = nn.LSTM(embed_dim,hidden_size,n_layers,dropout = (0 if n_layers == 1 else self.dropout),batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_step,last_hidden):\n",
    "        #input_step (batch_size,seq_len)\n",
    "        embedded = self.embedding(input_step)\n",
    "        #embedded(batch_size,seq_len,hidden_dim)\n",
    "\n",
    "        output,hidden = self.LSTM(embedded,last_hidden)\n",
    "        #output(batch_size,seq_len,hidden_dim)\n",
    "        #seq_len = 1 if we using teacher forcing\n",
    "        #output = output.squeeze(1) #(batch_size,hidden_dim) for teacher forcing\n",
    "        \n",
    "        prediction = self.out(output)\n",
    "        #prediction(seq_len,batch_size,output_dim) if no teacher_forcing\n",
    "        \n",
    "        return prediction,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = encoder(kek[0].to(device))\n",
    "output,hidden = decoder(kek[1].to(device),hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(32,64,len(dataset.characters_vocab)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderLSTM(32,64,len(dataset.transcripts_vocab)).to(device)\n",
    "len(dataset.characters_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self,x,y,teacher_forcing_ratio = 0.3):\n",
    "        \n",
    "        encoder_output,hidden = self.encoder(x)\n",
    "        decoder_outputs,hidden = self.decoder(y,hidden)\n",
    "        \n",
    "       \n",
    "        #hidden = self.encoder(x)\n",
    "        #input_token = y[:,0]\n",
    "        \n",
    "        #for t in range(1,seq_len):\n",
    "         #   output,hidden = self.decoder(input_token,hidden)\n",
    "          #  outputs[t] = output\n",
    "           # teacher_force = random.random() < teacher_forcing_ratio\n",
    "            #top1 = output.max(1)[1]\n",
    "           # input_token = (y[:,t] if teacher_force else top1)\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \n",
    "        #batch_size = 1!\n",
    "        x.unsqueeze_(0)\n",
    "        encoder_outputs,hidden = self.encoder(x)\n",
    "        char_to_input = torch.LongTensor([[2]]).to(device)\n",
    "        preds = []\n",
    "        while True:\n",
    "            predictions, hidden = self.decoder(char_to_input, hidden)\n",
    "            index_of_next = torch.argmax(predictions)\n",
    "            our_value = index_of_next.item()\n",
    "            if our_value == 3:\n",
    "                break\n",
    "            preds.append(our_value)\n",
    "            char_to_input = index_of_next.unsqueeze_(0).unsqueeze_(0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq(encoder,decoder,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,hidden = encoder(x)\n",
    "out_input = torch.LongTensor([[2]]).to(device)\n",
    "preds = []\n",
    "\n",
    "output,hidden = decoder(out_input,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.argmax(output)\n",
    "output.unsqueeze_(0).unsqueeze_(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): EncoderLSTM(\n",
       "    (embedding): Embedding(28, 32, padding_idx=0)\n",
       "    (LSTM): LSTM(32, 64, batch_first=True)\n",
       "  )\n",
       "  (decoder): DecoderLSTM(\n",
       "    (embedding): Embedding(28, 32, padding_idx=0)\n",
       "    (LSTM): LSTM(32, 64, batch_first=True)\n",
       "    (out): Linear(in_features=64, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 53,788 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<sos>': 2, '<eos>': 3, '<pad>': 0, '<unk>': 1, 'M': 4, 'J': 5, 'W': 6, 'D': 7, 'T': 8, 'H': 9, 'L': 10, 'C': 11, 'R': 12, 'S': 13, 'Y': 14, 'I': 15, 'Z': 16, 'K': 17, 'O': 18, 'F': 19, 'G': 20, 'P': 21, 'V': 22, 'U': 23, 'E': 24, 'A': 25, 'N': 26, 'B': 27}"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.transcripts_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,epoch,train_loss_list):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        x = batch[0].to(device)\n",
    "        y_in = batch[1].to(device)\n",
    "        y_out = batch[2].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x, y_in)\n",
    "        #output dim (y_seq_len,batch_size,output_dim)\n",
    "        output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "        y_out = y_out.view(-1)\n",
    "        \n",
    "        loss = criterion(output, y_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            plot(epoch, i, train_loss_list)\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            y_in = batch[1].to(device)\n",
    "            y_out = batch[2].to(device)\n",
    "            \n",
    "            output = model(x,y_in,0) #turn off teacher forcing\n",
    "            \n",
    "            output = output.view(output.shape[0]*output.shape[1],-1)\n",
    "            y_out = y_out.view(-1)\n",
    "\n",
    "            loss = criterion(output, y_out)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "def plot(epoch, step, train_losses):\n",
    "    clear_output(True)\n",
    "    plt.title(f'Epochs {epoch}, step {step}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP,epoch,train_losses)\n",
    "    valid_loss = evaluate(model, validation_loader, criterion)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'encoder_weights')\n",
    "torch.save(decoder.state_dict(),'decoder_weights')\n",
    "torch.save(model.state_dict(),'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('encoder_weights'))\n",
    "decoder.load_state_dict(torch.load('decoder_weights'))\n",
    "model = seq2seq(encoder,decoder,device)\n",
    "model.load_state_dict(torch.load('model_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model,batch,batch_size):\n",
    "    x_tokens = []\n",
    "    y_tokens_predicted = []\n",
    "    y_tokens_true = []\n",
    "    for i in range(batch_size):\n",
    "        x_tokens.append(''.join([dataset.characters_vocab.idx2token(k.item()) for k in batch[0][i] if k.item() != 0]))\n",
    "        y_pred = model.predict(batch[0][i].to(device))\n",
    "        y_tokens_predicted.append(''.join([dataset.transcripts_vocab.idx2token(k) for k in y_pred]))\n",
    "        y_tokens_true.append(''.join([dataset.transcripts_vocab.idx2token(k.item()) for k in batch[1][i] if k.item() != 0 and k.item() != 2]))\n",
    "    for i in range(len(x_tokens)):\n",
    "        print('Input:',x_tokens[i])\n",
    "        print('Predicted_output:',y_tokens_predicted[i])\n",
    "        print('True output:',y_tokens_true[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: CATAWBA\n",
      "Predicted_output: KAETAHBAO\n",
      "True output: KAHTAOBAH \n",
      "\n",
      "\n",
      "Input: VERIFY\n",
      "Predicted_output: VERIHFIY\n",
      "True output: VEHRAHFAY \n",
      "\n",
      "\n",
      "Input: MANEUVERINGS\n",
      "Predicted_output: MAHNIHVAHNBIHNZ\n",
      "True output: MAHNUWVERIHNGZ \n",
      "\n",
      "\n",
      "Input: FALLIS\n",
      "Predicted_output: FAELIHS\n",
      "True output: FAELIHS \n",
      "\n",
      "\n",
      "Input: OLEOYL\n",
      "Predicted_output: OWLIYAHL\n",
      "True output: OWLIYOYL \n",
      "\n",
      "\n",
      "Input: BLAMES\n",
      "Predicted_output: BLEYMZ\n",
      "True output: BLEYMZ \n",
      "\n",
      "\n",
      "Input: HOODLUMS\n",
      "Predicted_output: HHUWDLAHMZ\n",
      "True output: HHUWDLAHMZ \n",
      "\n",
      "\n",
      "Input: BOXES\n",
      "Predicted_output: BAAKSIHZ\n",
      "True output: BAAKSIHZ \n",
      "\n",
      "\n",
      "Input: FELLHEIMER\n",
      "Predicted_output: FEHLHHIYLERD\n",
      "True output: FEHLHHAYMER \n",
      "\n",
      "\n",
      "Input: GLOATS\n",
      "Predicted_output: GLOWTS\n",
      "True output: GLOWTS \n",
      "\n",
      "\n",
      "Input: ACHILLES\n",
      "Predicted_output: AECHAHLZ\n",
      "True output: AHKIHLIYZ \n",
      "\n",
      "\n",
      "Input: CUMINGS\n",
      "Predicted_output: KUWMIHNGZ\n",
      "True output: KUWMIHNGZ \n",
      "\n",
      "\n",
      "Input: SHAPERO\n",
      "Predicted_output: SHEYPEHROW\n",
      "True output: SHAHPIHROW \n",
      "\n",
      "\n",
      "Input: SHEAF\n",
      "Predicted_output: SHIYF\n",
      "True output: SHIYF \n",
      "\n",
      "\n",
      "Input: SHAY\n",
      "Predicted_output: SHIY\n",
      "True output: SHEY \n",
      "\n",
      "\n",
      "Input: IRRATIONALLY\n",
      "Predicted_output: IHRAHTIHNGLIYOW\n",
      "True output: IHRAESHAHNAHLIY \n",
      "\n",
      "\n",
      "Input: ARAGON\n",
      "Predicted_output: AARAHGAHN\n",
      "True output: EHRAHGAAN \n",
      "\n",
      "\n",
      "Input: ECO\n",
      "Predicted_output: IYKOW\n",
      "True output: EHKOW \n",
      "\n",
      "\n",
      "Input: PUSHUP\n",
      "Predicted_output: PUWSHAHP\n",
      "True output: PUHSHAHP \n",
      "\n",
      "\n",
      "Input: WORKHORSES\n",
      "Predicted_output: WERKHHAORS\n",
      "True output: WERKHHAORSIHZ \n",
      "\n",
      "\n",
      "Input: TRANG\n",
      "Predicted_output: TRAENG\n",
      "True output: TRAENG \n",
      "\n",
      "\n",
      "Input: STYLISHLY\n",
      "Predicted_output: STAYLZIYAHLIY\n",
      "True output: STAYLIHSHLIY \n",
      "\n",
      "\n",
      "Input: BARBIES\n",
      "Predicted_output: BAARBIYZ\n",
      "True output: BAARBIYZ \n",
      "\n",
      "\n",
      "Input: ZIEGENHORN\n",
      "Predicted_output: ZIYGAHNHHAARN\n",
      "True output: ZIYGIHNHHERN \n",
      "\n",
      "\n",
      "Input: EXTRAGALACTIC\n",
      "Predicted_output: EHKSTRAHSHAHBAHSIHNG\n",
      "True output: EHKSTRAHGAHLAEKTIHK \n",
      "\n",
      "\n",
      "Input: ELEGANCE\n",
      "Predicted_output: IHLIHGAHNS\n",
      "True output: EHLAHGAHNS \n",
      "\n",
      "\n",
      "Input: KARPOWICZ\n",
      "Predicted_output: KAARPAHWIHKS\n",
      "True output: KAARPAHVIHCH \n",
      "\n",
      "\n",
      "Input: ALBERTI\n",
      "Predicted_output: AELBERTIY\n",
      "True output: AALBEHRTIY \n",
      "\n",
      "\n",
      "Input: ANDREASEN\n",
      "Predicted_output: AENDRIHSHAHLZ\n",
      "True output: AENDRIYSAHN \n",
      "\n",
      "\n",
      "Input: ELECTRODES\n",
      "Predicted_output: IHLEHKTRAHDZ\n",
      "True output: IHLEHKTROWDZ \n",
      "\n",
      "\n",
      "Input: TRAFFIC\n",
      "Predicted_output: TRAEFIHK\n",
      "True output: TRAEFIHK \n",
      "\n",
      "\n",
      "Input: KEYED\n",
      "Predicted_output: KIYD\n",
      "True output: KIYD \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_batch(model,kek,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CATAWBA<pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<sos>KAHTAOBAH<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>')"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = ''\n",
    "out = ''\n",
    "for i in x: \n",
    "    inp += dataset.characters_vocab.idx2token(i.item())\n",
    "for i in y:\n",
    "    out += dataset.transcripts_vocab.idx2token(i.item())\n",
    "inp,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kek[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-460-2430716c7d3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "x_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
